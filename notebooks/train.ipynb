{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append('../common')\n",
    "from dataset import TMH\n",
    "from models import CNN\n",
    "from cait_models import cait_S24_224, cait_tmh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset_path = \"/Users/fga/data/tmh\"\n",
    "dataset = TMH(embeddings_path=dataset_path+\"/embeddings.h5\",\n",
    "                protein_hashes_path=dataset_path+\"/seq_anno_hash.pickle\",\n",
    "                train_ids=dataset_path+\"/data_splits/train_prot_id_labels.csv\")\n",
    "dataloader = DataLoader(dataset, batch_size=4)\n",
    "\n",
    "test_dataset = TMH(embeddings_path=dataset_path+\"/embeddings.h5\",\n",
    "                protein_hashes_path=dataset_path+\"/seq_anno_hash.pickle\",\n",
    "                train_ids=dataset_path+\"/data_splits/test_prot_id_labels.csv\")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# net = CNN()\n",
    "# net = MLP()\n",
    "# net = cait_S24_224()\n",
    "net = cait_tmh()\n",
    "\n",
    "# The function is indicating the performance of the model.\n",
    "# During the training process this function should be minimized\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# The minimization is achieved through Stochastic Gradient Descent\n",
    "optimizer = optim.Adagrad(net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4876, -0.5640, -0.7576, -0.3238],\n",
      "        [ 0.4875, -0.5639, -0.7575, -0.3241],\n",
      "        [ 0.4877, -0.5640, -0.7576, -0.3238],\n",
      "        [ 0.4877, -0.5640, -0.7576, -0.3239]], grad_fn=<AddmmBackward0>) \n",
      " tensor([[0, 1, 0, 0],\n",
      "        [0, 0, 1, 0],\n",
      "        [0, 1, 0, 0],\n",
      "        [1, 0, 0, 0]]) \n",
      " 1.5701520442962646\n"
     ]
    }
   ],
   "source": [
    "# single step\n",
    "inputs, labels = iter(dataloader).__next__()\n",
    "\n",
    "x = inputs.unsqueeze(1)\n",
    "\n",
    "try:\n",
    "    outputs = net(x)\n",
    "except ValueError:\n",
    "    import ipdb\n",
    "    ipdb.post_mortem()\n",
    "\n",
    "loss = criterion(outputs, labels.float())\n",
    "\n",
    "print(f\"{outputs} \\n {labels} \\n {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 100] loss: 1.853\n",
      "[1, 200] loss: 1.129\n",
      "[1, 300] loss: 1.098\n",
      "[1, 400] loss: 1.143\n",
      "[1, 500] loss: 1.186\n",
      "[1, 600] loss: 1.182\n",
      "[1, 700] loss: 1.116\n",
      "[1, 800] loss: 1.134\n",
      "[2, 100] loss: 1.111\n",
      "[2, 200] loss: 1.059\n",
      "[2, 300] loss: 1.072\n",
      "[2, 400] loss: 1.115\n",
      "[2, 500] loss: 1.168\n",
      "[2, 600] loss: 1.166\n",
      "[2, 700] loss: 1.104\n",
      "[2, 800] loss: 1.124\n",
      "[3, 100] loss: 1.099\n",
      "[3, 200] loss: 0.915\n",
      "[3, 300] loss: 0.886\n",
      "[3, 400] loss: 1.098\n",
      "[3, 500] loss: 1.093\n",
      "[3, 600] loss: 1.096\n",
      "[3, 700] loss: 1.059\n",
      "[3, 800] loss: 0.745\n",
      "[4, 100] loss: 0.511\n",
      "[4, 200] loss: 0.435\n",
      "[4, 300] loss: 0.475\n",
      "[4, 400] loss: 0.479\n",
      "[4, 500] loss: 0.504\n",
      "[4, 600] loss: 0.501\n",
      "[4, 700] loss: 0.417\n",
      "[4, 800] loss: 0.441\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(4):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        \"\"\"\n",
    "        shape of inputs: torch.Size([4, 3, 32, 32])\n",
    "            Batchsize: 4\n",
    "            Channels: 3 (Red, Green, Blue)\n",
    "            Image size: 32 x 32\n",
    "\n",
    "        labels: tensor([9, 3, 0, 3])\n",
    "            9: class of image 0 in batch\n",
    "            3: class of image 1 in batch\n",
    "            ...\n",
    "        \"\"\"\n",
    "        inputs, labels = data\n",
    "\n",
    "        \"\"\" zero the parameter gradients after every batch\n",
    "        This is necessary because the gradients (directions of how the weigths and biases\n",
    "        will be updated) are accumulated in each backward pass.\n",
    "        https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch\n",
    "        \"\"\"\n",
    "        optimizer.zero_grad()  # SGD\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        # shape outputs: torch.Size([4, 10])\n",
    "        # for every image a prediction\n",
    "        # print(f\"{inputs}\")\n",
    "        outputs = net(inputs.unsqueeze(1))\n",
    "\n",
    "        #print(f\"{outputs} \\t {labels}\")\n",
    "\n",
    "        # the first iteration CrossEntropy: tensor(2.3100, grad_fn=<NllLossBackward0>)\n",
    "\n",
    "        loss = criterion(outputs, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # running loss after 3 iterations: 6.894119024276733\n",
    "        # Why is the loss added?\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1}] loss: {(running_loss / 99):.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Evaluate the model on the test data\n",
    "This could be done with TorchMetrics but we will do this manually here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 91 test embeddings: 86.43\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images.unsqueeze(1))\n",
    "\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, labels = torch.max(labels.data, 1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the {len(test_dataloader)} test embeddings: {(100 * correct / total):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class G_SP  is: 82.6 %\n",
      "Accuracy for class G     is: 99.5 %\n",
      "Accuracy for class SP_TM is: 72.7 %\n",
      "Accuracy for class TM    is: 0.0 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nAccuracy for class G_SP  is: 93.2 %\\nAccuracy for class G     is: 99.4 %\\nAccuracy for class SP_TM is: 87.6 %\\nAccuracy for class TM    is: 93.5 %\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare to count predictions for each class\n",
    "mapp = {\n",
    "            'G_SP': 0,\n",
    "            'G': 1,\n",
    "            'SP_TM': 2,\n",
    "            'TM': 3\n",
    "        }\n",
    "classes = list(mapp.keys())\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images.unsqueeze(1))\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        _, labels = torch.max(labels, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(\"Accuracy for class {:5s} is: {:.1f} %\".format(classname,\n",
    "                                                         accuracy))\n",
    "\n",
    "\"\"\"\n",
    "Accuracy for class G_SP  is: 93.2 %\n",
    "Accuracy for class G     is: 99.4 %\n",
    "Accuracy for class SP_TM is: 87.6 %\n",
    "Accuracy for class TM    is: 93.5 %\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Create hashsum"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
