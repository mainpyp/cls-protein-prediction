{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This is official pytorch tutorial: <a href=https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py> Blitz Tutorial<a>\n",
    "\n",
    "What is done in this tutorial:\n",
    "    1. Load and normalize the CIFAR10 training and test datasets using torchvision\n",
    "    2. Define a Convolutional Neural Network\n",
    "    3. Define a loss function\n",
    "    4. Train the network on the training data\n",
    "    5. Test the network on the test data\n",
    "\n",
    "TORCHVISION:\n",
    "    The torchvision package consists of popular datasets, model architectures,\n",
    "    and common image transformations for computer vision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "embeddings = h5py.File(\"../../data/embeddings.h5\", \"r\")\n",
    "with open(\"../../data/seq_anno_hash.pickle\", 'rb') as handle:\n",
    "    proteins_and_hashes = pickle.load(handle)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3246\n",
      "1392\n"
     ]
    },
    {
     "data": {
      "text/plain": "     prot_id  label\n3831  O16299      G\n2899  P03524  SP_TM\n2165  B6EU02      G\n4334  Q95UE8   G_SP\n3760  Q0VD86      G\n...      ...    ...\n3631  Q8BXQ2  SP_TM\n1467  Q8I7Z8      G\n576   P51946      G\n433   Q12211      G\n1846  P09893   G_SP\n\n[3246 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prot_id</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3831</th>\n      <td>O16299</td>\n      <td>G</td>\n    </tr>\n    <tr>\n      <th>2899</th>\n      <td>P03524</td>\n      <td>SP_TM</td>\n    </tr>\n    <tr>\n      <th>2165</th>\n      <td>B6EU02</td>\n      <td>G</td>\n    </tr>\n    <tr>\n      <th>4334</th>\n      <td>Q95UE8</td>\n      <td>G_SP</td>\n    </tr>\n    <tr>\n      <th>3760</th>\n      <td>Q0VD86</td>\n      <td>G</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3631</th>\n      <td>Q8BXQ2</td>\n      <td>SP_TM</td>\n    </tr>\n    <tr>\n      <th>1467</th>\n      <td>Q8I7Z8</td>\n      <td>G</td>\n    </tr>\n    <tr>\n      <th>576</th>\n      <td>P51946</td>\n      <td>G</td>\n    </tr>\n    <tr>\n      <th>433</th>\n      <td>Q12211</td>\n      <td>G</td>\n    </tr>\n    <tr>\n      <th>1846</th>\n      <td>P09893</td>\n      <td>G_SP</td>\n    </tr>\n  </tbody>\n</table>\n<p>3246 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_prot = pd.read_csv(\"../data_splits/train_prot_id_labels.csv\")\n",
    "\n",
    "X_train, X_test = train_test_split(label_prot, test_size = 0.3, train_size=0.7, random_state=42, stratify=label_prot[\"label\"])\n",
    "print(len(X_train))\n",
    "print(len(X_test))\n",
    "X_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "train_embeddings = list()\n",
    "train_labels = list()\n",
    "train_protein_ids = list()\n",
    "for index, row in X_train.iterrows():\n",
    "    hash_code = proteins_and_hashes[row[\"label\"]][row[\"prot_id\"]][2]\n",
    "    mean_embedding = np.mean(embeddings.get(hash_code), axis=0)\n",
    "    label = row[\"label\"]\n",
    "    id = row[\"prot_id\"]\n",
    "    train_protein_ids.append(id)\n",
    "    train_embeddings.append(mean_embedding)\n",
    "    train_labels.append(label)\n",
    "\n",
    "test_embeddings = list()\n",
    "test_labels = list()\n",
    "test_protein_ids = list()\n",
    "for index, row in X_test.iterrows():\n",
    "    hash_code = proteins_and_hashes[row[\"label\"]][row[\"prot_id\"]][2]\n",
    "    mean_embedding = np.mean(embeddings.get(hash_code), axis=0)\n",
    "    label = row[\"label\"]\n",
    "    id = row[\"prot_id\"]\n",
    "    test_protein_ids.append(id)\n",
    "    test_embeddings.append(mean_embedding)\n",
    "    test_labels.append(label)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "             0         1         2         3         4         5         6  \\\n0    -0.011368 -0.041199  0.004509  0.024597  0.006310  0.011063 -0.057220   \n1    -0.017212 -0.026138  0.082520 -0.003960  0.007973  0.053406 -0.015556   \n2    -0.093140 -0.068115 -0.028992  0.065491  0.003998  0.018326 -0.051361   \n3     0.046814  0.033203  0.031403  0.009018  0.001868  0.009888 -0.074585   \n4     0.016586  0.013885  0.028610  0.000306  0.017242  0.044983 -0.026459   \n...        ...       ...       ...       ...       ...       ...       ...   \n1387  0.026260 -0.013016  0.033203 -0.011421  0.005230  0.013847 -0.043030   \n1388  0.019928 -0.016663  0.018845  0.033356  0.008423  0.035187  0.010178   \n1389  0.002785  0.002302 -0.010139  0.031113 -0.036102  0.043854 -0.084900   \n1390  0.004662 -0.009460 -0.027359 -0.012222 -0.012253 -0.004524 -0.063599   \n1391 -0.008331  0.011971  0.015045 -0.025513 -0.051300  0.075684 -0.038025   \n\n             7         8         9  ...      1015      1016      1017  \\\n0    -0.078491  0.002256 -0.007217  ... -0.009239  0.001315 -0.018234   \n1    -0.048523  0.056610  0.052582  ... -0.009422 -0.006477 -0.013466   \n2    -0.077942  0.014542 -0.000029  ... -0.039612 -0.049194  0.011292   \n3    -0.052277  0.017136 -0.023804  ... -0.011879  0.000846 -0.009827   \n4    -0.036957  0.064697 -0.024429  ... -0.013718 -0.033630  0.031128   \n...        ...       ...       ...  ...       ...       ...       ...   \n1387 -0.079651  0.033112  0.031113  ...  0.027863 -0.007492 -0.075806   \n1388 -0.074585  0.030396  0.044067  ... -0.039124 -0.008720 -0.030411   \n1389 -0.040833  0.008842 -0.025436  ... -0.025436 -0.018982 -0.045197   \n1390 -0.046326 -0.025650 -0.029236  ... -0.037659  0.005707 -0.043182   \n1391 -0.054718  0.003366  0.014717  ...  0.008240 -0.008400 -0.015320   \n\n          1018      1019      1020      1021      1022      1023  labels  \n0     0.030777  0.043732 -0.015915 -0.052979  0.040161  0.048889       G  \n1     0.028137 -0.014320 -0.001544 -0.044952  0.050262  0.085754   SP_TM  \n2     0.029770  0.032501 -0.025558  0.030136 -0.006653  0.068604       G  \n3     0.071533  0.014061  0.012444  0.044098  0.045319  0.014381    G_SP  \n4     0.047241  0.013695 -0.014023  0.012863 -0.007317  0.027237       G  \n...        ...       ...       ...       ...       ...       ...     ...  \n1387  0.035034  0.022888 -0.008621 -0.001949  0.025421  0.054199      TM  \n1388  0.018265  0.064941  0.031647  0.006977  0.070618  0.019775    G_SP  \n1389  0.049438  0.025604 -0.065735 -0.019043 -0.020721  0.025833       G  \n1390  0.023422  0.027954  0.006458 -0.044891  0.008850  0.041687       G  \n1391  0.029968  0.036438  0.026276 -0.008141  0.025391  0.092712       G  \n\n[1392 rows x 1025 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>1015</th>\n      <th>1016</th>\n      <th>1017</th>\n      <th>1018</th>\n      <th>1019</th>\n      <th>1020</th>\n      <th>1021</th>\n      <th>1022</th>\n      <th>1023</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.011368</td>\n      <td>-0.041199</td>\n      <td>0.004509</td>\n      <td>0.024597</td>\n      <td>0.006310</td>\n      <td>0.011063</td>\n      <td>-0.057220</td>\n      <td>-0.078491</td>\n      <td>0.002256</td>\n      <td>-0.007217</td>\n      <td>...</td>\n      <td>-0.009239</td>\n      <td>0.001315</td>\n      <td>-0.018234</td>\n      <td>0.030777</td>\n      <td>0.043732</td>\n      <td>-0.015915</td>\n      <td>-0.052979</td>\n      <td>0.040161</td>\n      <td>0.048889</td>\n      <td>G</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.017212</td>\n      <td>-0.026138</td>\n      <td>0.082520</td>\n      <td>-0.003960</td>\n      <td>0.007973</td>\n      <td>0.053406</td>\n      <td>-0.015556</td>\n      <td>-0.048523</td>\n      <td>0.056610</td>\n      <td>0.052582</td>\n      <td>...</td>\n      <td>-0.009422</td>\n      <td>-0.006477</td>\n      <td>-0.013466</td>\n      <td>0.028137</td>\n      <td>-0.014320</td>\n      <td>-0.001544</td>\n      <td>-0.044952</td>\n      <td>0.050262</td>\n      <td>0.085754</td>\n      <td>SP_TM</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.093140</td>\n      <td>-0.068115</td>\n      <td>-0.028992</td>\n      <td>0.065491</td>\n      <td>0.003998</td>\n      <td>0.018326</td>\n      <td>-0.051361</td>\n      <td>-0.077942</td>\n      <td>0.014542</td>\n      <td>-0.000029</td>\n      <td>...</td>\n      <td>-0.039612</td>\n      <td>-0.049194</td>\n      <td>0.011292</td>\n      <td>0.029770</td>\n      <td>0.032501</td>\n      <td>-0.025558</td>\n      <td>0.030136</td>\n      <td>-0.006653</td>\n      <td>0.068604</td>\n      <td>G</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.046814</td>\n      <td>0.033203</td>\n      <td>0.031403</td>\n      <td>0.009018</td>\n      <td>0.001868</td>\n      <td>0.009888</td>\n      <td>-0.074585</td>\n      <td>-0.052277</td>\n      <td>0.017136</td>\n      <td>-0.023804</td>\n      <td>...</td>\n      <td>-0.011879</td>\n      <td>0.000846</td>\n      <td>-0.009827</td>\n      <td>0.071533</td>\n      <td>0.014061</td>\n      <td>0.012444</td>\n      <td>0.044098</td>\n      <td>0.045319</td>\n      <td>0.014381</td>\n      <td>G_SP</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.016586</td>\n      <td>0.013885</td>\n      <td>0.028610</td>\n      <td>0.000306</td>\n      <td>0.017242</td>\n      <td>0.044983</td>\n      <td>-0.026459</td>\n      <td>-0.036957</td>\n      <td>0.064697</td>\n      <td>-0.024429</td>\n      <td>...</td>\n      <td>-0.013718</td>\n      <td>-0.033630</td>\n      <td>0.031128</td>\n      <td>0.047241</td>\n      <td>0.013695</td>\n      <td>-0.014023</td>\n      <td>0.012863</td>\n      <td>-0.007317</td>\n      <td>0.027237</td>\n      <td>G</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1387</th>\n      <td>0.026260</td>\n      <td>-0.013016</td>\n      <td>0.033203</td>\n      <td>-0.011421</td>\n      <td>0.005230</td>\n      <td>0.013847</td>\n      <td>-0.043030</td>\n      <td>-0.079651</td>\n      <td>0.033112</td>\n      <td>0.031113</td>\n      <td>...</td>\n      <td>0.027863</td>\n      <td>-0.007492</td>\n      <td>-0.075806</td>\n      <td>0.035034</td>\n      <td>0.022888</td>\n      <td>-0.008621</td>\n      <td>-0.001949</td>\n      <td>0.025421</td>\n      <td>0.054199</td>\n      <td>TM</td>\n    </tr>\n    <tr>\n      <th>1388</th>\n      <td>0.019928</td>\n      <td>-0.016663</td>\n      <td>0.018845</td>\n      <td>0.033356</td>\n      <td>0.008423</td>\n      <td>0.035187</td>\n      <td>0.010178</td>\n      <td>-0.074585</td>\n      <td>0.030396</td>\n      <td>0.044067</td>\n      <td>...</td>\n      <td>-0.039124</td>\n      <td>-0.008720</td>\n      <td>-0.030411</td>\n      <td>0.018265</td>\n      <td>0.064941</td>\n      <td>0.031647</td>\n      <td>0.006977</td>\n      <td>0.070618</td>\n      <td>0.019775</td>\n      <td>G_SP</td>\n    </tr>\n    <tr>\n      <th>1389</th>\n      <td>0.002785</td>\n      <td>0.002302</td>\n      <td>-0.010139</td>\n      <td>0.031113</td>\n      <td>-0.036102</td>\n      <td>0.043854</td>\n      <td>-0.084900</td>\n      <td>-0.040833</td>\n      <td>0.008842</td>\n      <td>-0.025436</td>\n      <td>...</td>\n      <td>-0.025436</td>\n      <td>-0.018982</td>\n      <td>-0.045197</td>\n      <td>0.049438</td>\n      <td>0.025604</td>\n      <td>-0.065735</td>\n      <td>-0.019043</td>\n      <td>-0.020721</td>\n      <td>0.025833</td>\n      <td>G</td>\n    </tr>\n    <tr>\n      <th>1390</th>\n      <td>0.004662</td>\n      <td>-0.009460</td>\n      <td>-0.027359</td>\n      <td>-0.012222</td>\n      <td>-0.012253</td>\n      <td>-0.004524</td>\n      <td>-0.063599</td>\n      <td>-0.046326</td>\n      <td>-0.025650</td>\n      <td>-0.029236</td>\n      <td>...</td>\n      <td>-0.037659</td>\n      <td>0.005707</td>\n      <td>-0.043182</td>\n      <td>0.023422</td>\n      <td>0.027954</td>\n      <td>0.006458</td>\n      <td>-0.044891</td>\n      <td>0.008850</td>\n      <td>0.041687</td>\n      <td>G</td>\n    </tr>\n    <tr>\n      <th>1391</th>\n      <td>-0.008331</td>\n      <td>0.011971</td>\n      <td>0.015045</td>\n      <td>-0.025513</td>\n      <td>-0.051300</td>\n      <td>0.075684</td>\n      <td>-0.038025</td>\n      <td>-0.054718</td>\n      <td>0.003366</td>\n      <td>0.014717</td>\n      <td>...</td>\n      <td>0.008240</td>\n      <td>-0.008400</td>\n      <td>-0.015320</td>\n      <td>0.029968</td>\n      <td>0.036438</td>\n      <td>0.026276</td>\n      <td>-0.008141</td>\n      <td>0.025391</td>\n      <td>0.092712</td>\n      <td>G</td>\n    </tr>\n  </tbody>\n</table>\n<p>1392 rows × 1025 columns</p>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create dataloader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "label_mappings = {\n",
    "    'G_SP': 0,\n",
    "     'G': 1,\n",
    "     'SP_TM': 2,\n",
    "     'TM': 3\n",
    "}\n",
    "reverse_label_mappings = {val: key for key, val in label_mappings.items()}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reverse_label_mappings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "type(train_embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "train_embeddings = torch.Tensor(np.array(train_embeddings))\n",
    "train_labels = torch.Tensor([label_mappings[label] for label in  train_labels])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "test_embeddings = torch.Tensor(np.array(test_embeddings))\n",
    "test_labels = torch.Tensor([label_mappings[label] for label in  test_labels])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "train_labels = one_hot(train_labels.to(torch.int64), 4)\n",
    "dataset = TensorDataset(train_embeddings, train_labels)\n",
    "dataloader = DataLoader(dataset, batch_size=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "test_labels = one_hot(test_labels.to(torch.int64), 4)\n",
    "test_dataset = TensorDataset(test_embeddings, test_labels)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\" Define a model with two convolution layers each followed by a Max Pooling layer\n",
    "    Then the embeddings are Flattened and forwarded into two FCls.\n",
    "    The last layer is a FCL with 10 neurons.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(1024, 512)  # in channels -> 16 * 5 ^ 2 (kernel size 5)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 256)\n",
    "        self.fc4 = nn.Linear(256, 4)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        #x = F.softmax(x, dim=4)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = MLP()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# The function is indicating the performance of the model.\n",
    "# During the training process this function should be minimized\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# The minimization is achieved through Stochastic Gradient Descent\n",
    "optimizer = optim.Adagrad(net.parameters(), lr=0.01)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 100] loss: 0.567\n",
      "[1, 200] loss: 0.303\n",
      "[1, 300] loss: 0.273\n",
      "[1, 400] loss: 0.182\n",
      "[1, 500] loss: 0.178\n",
      "[1, 600] loss: 0.119\n",
      "[1, 700] loss: 0.123\n",
      "[1, 800] loss: 0.158\n",
      "[2, 100] loss: 0.056\n",
      "[2, 200] loss: 0.080\n",
      "[2, 300] loss: 0.103\n",
      "[2, 400] loss: 0.072\n",
      "[2, 500] loss: 0.096\n",
      "[2, 600] loss: 0.054\n",
      "[2, 700] loss: 0.072\n",
      "[2, 800] loss: 0.065\n",
      "[3, 100] loss: 0.032\n",
      "[3, 200] loss: 0.045\n",
      "[3, 300] loss: 0.063\n",
      "[3, 400] loss: 0.038\n",
      "[3, 500] loss: 0.056\n",
      "[3, 600] loss: 0.031\n",
      "[3, 700] loss: 0.034\n",
      "[3, 800] loss: 0.022\n",
      "[4, 100] loss: 0.022\n",
      "[4, 200] loss: 0.017\n",
      "[4, 300] loss: 0.034\n",
      "[4, 400] loss: 0.027\n",
      "[4, 500] loss: 0.042\n",
      "[4, 600] loss: 0.024\n",
      "[4, 700] loss: 0.021\n",
      "[4, 800] loss: 0.015\n",
      "[5, 100] loss: 0.018\n",
      "[5, 200] loss: 0.010\n",
      "[5, 300] loss: 0.017\n",
      "[5, 400] loss: 0.015\n",
      "[5, 500] loss: 0.021\n",
      "[5, 600] loss: 0.014\n",
      "[5, 700] loss: 0.015\n",
      "[5, 800] loss: 0.006\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        \"\"\"\n",
    "        shape of inputs: torch.Size([4, 3, 32, 32])\n",
    "            Batchsize: 4\n",
    "            Channels: 3 (Red, Green, Blue)\n",
    "            Image size: 32 x 32\n",
    "\n",
    "        labels: tensor([9, 3, 0, 3])\n",
    "            9: class of image 0 in batch\n",
    "            3: class of image 1 in batch\n",
    "            ...\n",
    "        \"\"\"\n",
    "        inputs, labels = data\n",
    "\n",
    "        \"\"\" zero the parameter gradients after every batch\n",
    "        This is necessary because the gradients (directions of how the weigths and biases\n",
    "        will be updated) are accumulated in each backward pass.\n",
    "        https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch\n",
    "        \"\"\"\n",
    "        optimizer.zero_grad()  # SGD\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        # shape outputs: torch.Size([4, 10])\n",
    "        # for every image a prediction\n",
    "        #print(f\"{inputs}\")\n",
    "        outputs = net(inputs)\n",
    "        #print(f\"{outputs} \\t {labels}\")\n",
    "\n",
    "        # the first iteration CrossEntropy: tensor(2.3100, grad_fn=<NllLossBackward0>)\n",
    "        loss = criterion(outputs, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # running loss after 3 iterations: 6.894119024276733\n",
    "        # Why is the loss added?\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1}] loss: {(running_loss / 99):.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluate the model on the test data\n",
    "This could be done with TorchMetrics but we will do this manually here"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 1392 test embeddings: 97.70\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, labels = torch.max(labels.data, 1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the {len(test_labels)} test embeddings: {(100 * correct / total):.2f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class G_SP  is: 95.5 %\n",
      "Accuracy for class G     is: 99.7 %\n",
      "Accuracy for class SP_TM is: 91.7 %\n",
      "Accuracy for class TM    is: 100.0 %\n"
     ]
    },
    {
     "data": {
      "text/plain": "'\\nAccuracy for class G_SP  is: 97.7 %\\nAccuracy for class G     is: 99.3 %\\nAccuracy for class SP_TM is: 79.4 %\\nAccuracy for class TM    is: 86.2 %\\n'"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare to count predictions for each class\n",
    "classes = list(label_mappings.keys())\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        _, labels = torch.max(labels, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(\"Accuracy for class {:5s} is: {:.1f} %\".format(classname,\n",
    "                                                         accuracy))\n",
    "\n",
    "\"\"\"\n",
    "Accuracy for class G_SP  is: 97.7 %\n",
    "Accuracy for class G     is: 99.3 %\n",
    "Accuracy for class SP_TM is: 79.4 %\n",
    "Accuracy for class TM    is: 86.2 %\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create hashsum"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}