{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This is official pytorch tutorial: <a href=https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py> Blitz Tutorial<a>\n",
    "\n",
    "What is done in this tutorial:\n",
    "    1. Load and normalize the CIFAR10 training and test datasets using torchvision\n",
    "    2. Define a Convolutional Neural Network\n",
    "    3. Define a loss function\n",
    "    4. Train the network on the training data\n",
    "    5. Test the network on the test data\n",
    "\n",
    "TORCHVISION:\n",
    "    The torchvision package consists of popular datasets, model architectures,\n",
    "    and common image transformations for computer vision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pickle\n",
    "from itertools import chain\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "embeddings = h5py.File(\"../../data/embeddings.h5\", \"r\")\n",
    "with open(\"../../data/seq_anno_hash.pickle\", 'rb') as handle:\n",
    "    proteins_and_hashes = pickle.load(handle)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4638\n",
      "516\n"
     ]
    },
    {
     "data": {
      "text/plain": "     prot_id  label\n1240  P31716   G_SP\n2049  Q24537      G\n1660  Q9VWW0      G\n3495  Q0GGX2      G\n1260  P13423   G_SP\n...      ...    ...\n1801  P18146      G\n4276  Q21874  SP_TM\n2501  Q13415      G\n2687  Q9HGP0      G\n582   Q13093   G_SP\n\n[4638 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prot_id</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1240</th>\n      <td>P31716</td>\n      <td>G_SP</td>\n    </tr>\n    <tr>\n      <th>2049</th>\n      <td>Q24537</td>\n      <td>G</td>\n    </tr>\n    <tr>\n      <th>1660</th>\n      <td>Q9VWW0</td>\n      <td>G</td>\n    </tr>\n    <tr>\n      <th>3495</th>\n      <td>Q0GGX2</td>\n      <td>G</td>\n    </tr>\n    <tr>\n      <th>1260</th>\n      <td>P13423</td>\n      <td>G_SP</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1801</th>\n      <td>P18146</td>\n      <td>G</td>\n    </tr>\n    <tr>\n      <th>4276</th>\n      <td>Q21874</td>\n      <td>SP_TM</td>\n    </tr>\n    <tr>\n      <th>2501</th>\n      <td>Q13415</td>\n      <td>G</td>\n    </tr>\n    <tr>\n      <th>2687</th>\n      <td>Q9HGP0</td>\n      <td>G</td>\n    </tr>\n    <tr>\n      <th>582</th>\n      <td>Q13093</td>\n      <td>G_SP</td>\n    </tr>\n  </tbody>\n</table>\n<p>4638 rows Ã— 2 columns</p>\n</div>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_prot = [list(map(lambda x:[x,key], list(proteins_and_hashes[key].keys()))) for key in proteins_and_hashes]\n",
    "label_prot = list(chain.from_iterable(label_prot))\n",
    "label_prot = pd.DataFrame(label_prot, columns=[\"prot_id\", \"label\"])\n",
    "\n",
    "X_train, X_test = train_test_split(label_prot, test_size = 0.1, train_size=0.9, random_state=42, stratify=label_prot[\"label\"])\n",
    "print(len(X_train))\n",
    "print(len(X_test))\n",
    "X_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "train_embeddings = list()\n",
    "train_labels = list()\n",
    "train_protein_ids = list()\n",
    "for index, row in X_train.iterrows():\n",
    "    hash_code = proteins_and_hashes[row[\"label\"]][row[\"prot_id\"]][2]\n",
    "    mean_embedding = np.mean(embeddings.get(hash_code), axis=0)\n",
    "    label = row[\"label\"]\n",
    "    id = row[\"prot_id\"]\n",
    "    train_protein_ids.append(id)\n",
    "    train_embeddings.append(mean_embedding)\n",
    "    train_labels.append(label)\n",
    "\n",
    "test_embeddings = list()\n",
    "test_labels = list()\n",
    "test_protein_ids = list()\n",
    "for index, row in X_test.iterrows():\n",
    "    hash_code = proteins_and_hashes[row[\"label\"]][row[\"prot_id\"]][2]\n",
    "    mean_embedding = np.mean(embeddings.get(hash_code), axis=0)\n",
    "    label = row[\"label\"]\n",
    "    id = row[\"prot_id\"]\n",
    "    test_protein_ids.append(id)\n",
    "    test_embeddings.append(mean_embedding)\n",
    "    test_labels.append(label)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create dataloader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [],
   "source": [
    "label_mappings = {\n",
    "    'G_SP': 0,\n",
    "     'G': 1,\n",
    "     'SP_TM': 2,\n",
    "     'TM': 3\n",
    "}\n",
    "reverse_label_mappings = {val: key for key, val in label_mappings.items()}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [
    {
     "data": {
      "text/plain": "{0: 'G_SP', 1: 'G', 2: 'SP_TM', 3: 'TM'}"
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_label_mappings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "list"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "tensor(0.)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/hc/syymx7gj2rj7mmw3bymr8s0c0000gn/T/ipykernel_88197/2738016332.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mtrain_embeddings\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTensor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_embeddings\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mtrain_labels\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTensor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mlabel_mappings\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mlabel\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mlabel\u001B[0m \u001B[0;32min\u001B[0m  \u001B[0mtrain_labels\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0mdataset\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mTensorDataset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_embeddings\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_labels\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mdataloader\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mDataLoader\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/hc/syymx7gj2rj7mmw3bymr8s0c0000gn/T/ipykernel_88197/2738016332.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mtrain_embeddings\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTensor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_embeddings\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mtrain_labels\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTensor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mlabel_mappings\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mlabel\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mlabel\u001B[0m \u001B[0;32min\u001B[0m  \u001B[0mtrain_labels\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0mdataset\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mTensorDataset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_embeddings\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_labels\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mdataloader\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mDataLoader\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: tensor(0.)"
     ]
    }
   ],
   "source": [
    "train_embeddings = torch.Tensor(np.array(train_embeddings))\n",
    "train_labels = torch.Tensor([label_mappings[label] for label in  train_labels])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [],
   "source": [
    "test_embeddings = torch.Tensor(np.array(test_embeddings))\n",
    "test_labels = torch.Tensor([label_mappings[label] for label in  test_labels])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "train_labels = one_hot(train_labels.to(torch.int64), 4)\n",
    "dataset = TensorDataset(train_embeddings, train_labels)\n",
    "dataloader = DataLoader(dataset, batch_size=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [],
   "source": [
    "test_labels = one_hot(test_labels.to(torch.int64), 4)\n",
    "test_dataset = TensorDataset(test_embeddings, test_labels)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\" Define a model with two convolution layers each followed by a Max Pooling layer\n",
    "    Then the embeddings are Flattened and forwarded into two FCls.\n",
    "    The last layer is a FCL with 10 neurons.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(1024, 512)  # in channels -> 16 * 5 ^ 2 (kernel size 5)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 256)\n",
    "        self.fc4 = nn.Linear(256, 4)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        #x = F.softmax(x, dim=4)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = MLP()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# The function is indicating the performance of the model.\n",
    "# During the training process this function should be minimized\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# The minimization is achieved through Stochastic Gradient Descent\n",
    "optimizer = optim.Adagrad(net.parameters(), lr=0.01)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 100] loss: 0.595\n",
      "[1, 200] loss: 0.285\n",
      "[1, 300] loss: 0.199\n",
      "[1, 400] loss: 0.210\n",
      "[1, 500] loss: 0.151\n",
      "[1, 600] loss: 0.093\n",
      "[1, 700] loss: 0.080\n",
      "[1, 800] loss: 0.133\n",
      "[1, 900] loss: 0.186\n",
      "[1, 1000] loss: 0.175\n",
      "[1, 1100] loss: 0.084\n",
      "[2, 100] loss: 0.112\n",
      "[2, 200] loss: 0.060\n",
      "[2, 300] loss: 0.048\n",
      "[2, 400] loss: 0.073\n",
      "[2, 500] loss: 0.078\n",
      "[2, 600] loss: 0.036\n",
      "[2, 700] loss: 0.044\n",
      "[2, 800] loss: 0.063\n",
      "[2, 900] loss: 0.081\n",
      "[2, 1000] loss: 0.102\n",
      "[2, 1100] loss: 0.050\n",
      "[3, 100] loss: 0.084\n",
      "[3, 200] loss: 0.033\n",
      "[3, 300] loss: 0.025\n",
      "[3, 400] loss: 0.036\n",
      "[3, 500] loss: 0.045\n",
      "[3, 600] loss: 0.018\n",
      "[3, 700] loss: 0.028\n",
      "[3, 800] loss: 0.035\n",
      "[3, 900] loss: 0.040\n",
      "[3, 1000] loss: 0.058\n",
      "[3, 1100] loss: 0.033\n",
      "[4, 100] loss: 0.062\n",
      "[4, 200] loss: 0.016\n",
      "[4, 300] loss: 0.012\n",
      "[4, 400] loss: 0.019\n",
      "[4, 500] loss: 0.027\n",
      "[4, 600] loss: 0.009\n",
      "[4, 700] loss: 0.016\n",
      "[4, 800] loss: 0.021\n",
      "[4, 900] loss: 0.023\n",
      "[4, 1000] loss: 0.028\n",
      "[4, 1100] loss: 0.018\n",
      "[5, 100] loss: 0.043\n",
      "[5, 200] loss: 0.009\n",
      "[5, 300] loss: 0.008\n",
      "[5, 400] loss: 0.009\n",
      "[5, 500] loss: 0.017\n",
      "[5, 600] loss: 0.005\n",
      "[5, 700] loss: 0.008\n",
      "[5, 800] loss: 0.011\n",
      "[5, 900] loss: 0.012\n",
      "[5, 1000] loss: 0.009\n",
      "[5, 1100] loss: 0.011\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        \"\"\"\n",
    "        shape of inputs: torch.Size([4, 3, 32, 32])\n",
    "            Batchsize: 4\n",
    "            Channels: 3 (Red, Green, Blue)\n",
    "            Image size: 32 x 32\n",
    "\n",
    "        labels: tensor([9, 3, 0, 3])\n",
    "            9: class of image 0 in batch\n",
    "            3: class of image 1 in batch\n",
    "            ...\n",
    "        \"\"\"\n",
    "        inputs, labels = data\n",
    "\n",
    "        \"\"\" zero the parameter gradients after every batch\n",
    "        This is necessary because the gradients (directions of how the weigths and biases\n",
    "        will be updated) are accumulated in each backward pass.\n",
    "        https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch\n",
    "        \"\"\"\n",
    "        optimizer.zero_grad()  # SGD\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        # shape outputs: torch.Size([4, 10])\n",
    "        # for every image a prediction\n",
    "        #print(f\"{inputs}\")\n",
    "        outputs = net(inputs)\n",
    "        #print(f\"{outputs} \\t {labels}\")\n",
    "\n",
    "        # the first iteration CrossEntropy: tensor(2.3100, grad_fn=<NllLossBackward0>)\n",
    "        loss = criterion(outputs, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # running loss after 3 iterations: 6.894119024276733\n",
    "        # Why is the loss added?\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1}] loss: {(running_loss / 99):.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluate the model on the test data\n",
    "This could be done with TorchMetrics but we will do this manually here"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 516 test embeddings: 95.74\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, labels = torch.max(labels.data, 1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the {len(test_labels)} test embeddings: {(100 * correct / total):.2f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class G_SP  is: 97.7 %\n",
      "Accuracy for class G     is: 99.3 %\n",
      "Accuracy for class SP_TM is: 79.4 %\n",
      "Accuracy for class TM    is: 86.2 %\n"
     ]
    },
    {
     "data": {
      "text/plain": "'\\nAccuracy for class G_SP  is: 0.0 %\\nAccuracy for class G     is: 100.0 %\\nAccuracy for class SP_TM is: 0.0 %\\nAccuracy for class TM    is: 0.0 %\\n'"
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare to count predictions for each class\n",
    "classes = list(label_mappings.keys())\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        _, labels = torch.max(labels, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(\"Accuracy for class {:5s} is: {:.1f} %\".format(classname,\n",
    "                                                         accuracy))\n",
    "\n",
    "\"\"\"\n",
    "Accuracy for class G_SP  is: 0.0 %\n",
    "Accuracy for class G     is: 100.0 %\n",
    "Accuracy for class SP_TM is: 0.0 %\n",
    "Accuracy for class TM    is: 0.0 %\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create hashsum"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}