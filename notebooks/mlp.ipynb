{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This is official pytorch tutorial: <a href=https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py> Blitz Tutorial<a>\n",
    "\n",
    "What is done in this tutorial:\n",
    "    1. Load and normalize the CIFAR10 training and test datasets using torchvision\n",
    "    2. Define a Convolutional Neural Network\n",
    "    3. Define a loss function\n",
    "    4. Train the network on the training data\n",
    "    5. Test the network on the test data\n",
    "\n",
    "TORCHVISION:\n",
    "    The torchvision package consists of popular datasets, model architectures,\n",
    "    and common image transformations for computer vision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pickle\n",
    "from itertools import chain\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "embeddings = h5py.File(\"../../data/embeddings.h5\", \"r\")\n",
    "with open(\"../../data/seq_anno_hash.pickle\", 'rb') as handle:\n",
    "    proteins_and_hashes = pickle.load(handle)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3246\n",
      "1392\n"
     ]
    },
    {
     "data": {
      "text/plain": "     prot_id  label\n3831  O16299      G\n2899  P03524  SP_TM\n2165  B6EU02      G\n4334  Q95UE8   G_SP\n3760  Q0VD86      G\n...      ...    ...\n3631  Q8BXQ2  SP_TM\n1467  Q8I7Z8      G\n576   P51946      G\n433   Q12211      G\n1846  P09893   G_SP\n\n[3246 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prot_id</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3831</th>\n      <td>O16299</td>\n      <td>G</td>\n    </tr>\n    <tr>\n      <th>2899</th>\n      <td>P03524</td>\n      <td>SP_TM</td>\n    </tr>\n    <tr>\n      <th>2165</th>\n      <td>B6EU02</td>\n      <td>G</td>\n    </tr>\n    <tr>\n      <th>4334</th>\n      <td>Q95UE8</td>\n      <td>G_SP</td>\n    </tr>\n    <tr>\n      <th>3760</th>\n      <td>Q0VD86</td>\n      <td>G</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3631</th>\n      <td>Q8BXQ2</td>\n      <td>SP_TM</td>\n    </tr>\n    <tr>\n      <th>1467</th>\n      <td>Q8I7Z8</td>\n      <td>G</td>\n    </tr>\n    <tr>\n      <th>576</th>\n      <td>P51946</td>\n      <td>G</td>\n    </tr>\n    <tr>\n      <th>433</th>\n      <td>Q12211</td>\n      <td>G</td>\n    </tr>\n    <tr>\n      <th>1846</th>\n      <td>P09893</td>\n      <td>G_SP</td>\n    </tr>\n  </tbody>\n</table>\n<p>3246 rows Ã— 2 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_prot = pd.read_csv(\"../data_splits/train_prot_id_labels.csv\")\n",
    "\n",
    "X_train, X_test = train_test_split(label_prot, test_size = 0.3, train_size=0.7, random_state=42, stratify=label_prot[\"label\"])\n",
    "print(len(X_train))\n",
    "print(len(X_test))\n",
    "X_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "train_embeddings = list()\n",
    "train_labels = list()\n",
    "train_protein_ids = list()\n",
    "for index, row in X_train.iterrows():\n",
    "    hash_code = proteins_and_hashes[row[\"label\"]][row[\"prot_id\"]][2]\n",
    "    mean_embedding = np.mean(embeddings.get(hash_code), axis=0)\n",
    "    label = row[\"label\"]\n",
    "    id = row[\"prot_id\"]\n",
    "    train_protein_ids.append(id)\n",
    "    train_embeddings.append(mean_embedding)\n",
    "    train_labels.append(label)\n",
    "\n",
    "test_embeddings = list()\n",
    "test_labels = list()\n",
    "test_protein_ids = list()\n",
    "for index, row in X_test.iterrows():\n",
    "    hash_code = proteins_and_hashes[row[\"label\"]][row[\"prot_id\"]][2]\n",
    "    mean_embedding = np.mean(embeddings.get(hash_code), axis=0)\n",
    "    label = row[\"label\"]\n",
    "    id = row[\"prot_id\"]\n",
    "    test_protein_ids.append(id)\n",
    "    test_embeddings.append(mean_embedding)\n",
    "    test_labels.append(label)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create dataloader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "label_mappings = {\n",
    "    'G_SP': 0,\n",
    "     'G': 1,\n",
    "     'SP_TM': 2,\n",
    "     'TM': 3\n",
    "}\n",
    "reverse_label_mappings = {val: key for key, val in label_mappings.items()}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reverse_label_mappings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "type(train_embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "train_embeddings = torch.Tensor(np.array(train_embeddings))\n",
    "train_labels = torch.Tensor([label_mappings[label] for label in  train_labels])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "test_embeddings = torch.Tensor(np.array(test_embeddings))\n",
    "test_labels = torch.Tensor([label_mappings[label] for label in  test_labels])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "train_labels = one_hot(train_labels.to(torch.int64), 4)\n",
    "dataset = TensorDataset(train_embeddings, train_labels)\n",
    "dataloader = DataLoader(dataset, batch_size=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "test_labels = one_hot(test_labels.to(torch.int64), 4)\n",
    "test_dataset = TensorDataset(test_embeddings, test_labels)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\" Define a model with two convolution layers each followed by a Max Pooling layer\n",
    "    Then the embeddings are Flattened and forwarded into two FCls.\n",
    "    The last layer is a FCL with 10 neurons.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(1024, 512)  # in channels -> 16 * 5 ^ 2 (kernel size 5)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 256)\n",
    "        self.fc4 = nn.Linear(256, 4)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        #x = F.softmax(x, dim=4)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = MLP()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# The function is indicating the performance of the model.\n",
    "# During the training process this function should be minimized\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# The minimization is achieved through Stochastic Gradient Descent\n",
    "optimizer = optim.Adagrad(net.parameters(), lr=0.01)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 100] loss: 0.567\n",
      "[1, 200] loss: 0.303\n",
      "[1, 300] loss: 0.273\n",
      "[1, 400] loss: 0.182\n",
      "[1, 500] loss: 0.178\n",
      "[1, 600] loss: 0.119\n",
      "[1, 700] loss: 0.123\n",
      "[1, 800] loss: 0.158\n",
      "[2, 100] loss: 0.056\n",
      "[2, 200] loss: 0.080\n",
      "[2, 300] loss: 0.103\n",
      "[2, 400] loss: 0.072\n",
      "[2, 500] loss: 0.096\n",
      "[2, 600] loss: 0.054\n",
      "[2, 700] loss: 0.072\n",
      "[2, 800] loss: 0.065\n",
      "[3, 100] loss: 0.032\n",
      "[3, 200] loss: 0.045\n",
      "[3, 300] loss: 0.063\n",
      "[3, 400] loss: 0.038\n",
      "[3, 500] loss: 0.056\n",
      "[3, 600] loss: 0.031\n",
      "[3, 700] loss: 0.034\n",
      "[3, 800] loss: 0.022\n",
      "[4, 100] loss: 0.022\n",
      "[4, 200] loss: 0.017\n",
      "[4, 300] loss: 0.034\n",
      "[4, 400] loss: 0.027\n",
      "[4, 500] loss: 0.042\n",
      "[4, 600] loss: 0.024\n",
      "[4, 700] loss: 0.021\n",
      "[4, 800] loss: 0.015\n",
      "[5, 100] loss: 0.018\n",
      "[5, 200] loss: 0.010\n",
      "[5, 300] loss: 0.017\n",
      "[5, 400] loss: 0.015\n",
      "[5, 500] loss: 0.021\n",
      "[5, 600] loss: 0.014\n",
      "[5, 700] loss: 0.015\n",
      "[5, 800] loss: 0.006\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        \"\"\"\n",
    "        shape of inputs: torch.Size([4, 3, 32, 32])\n",
    "            Batchsize: 4\n",
    "            Channels: 3 (Red, Green, Blue)\n",
    "            Image size: 32 x 32\n",
    "\n",
    "        labels: tensor([9, 3, 0, 3])\n",
    "            9: class of image 0 in batch\n",
    "            3: class of image 1 in batch\n",
    "            ...\n",
    "        \"\"\"\n",
    "        inputs, labels = data\n",
    "\n",
    "        \"\"\" zero the parameter gradients after every batch\n",
    "        This is necessary because the gradients (directions of how the weigths and biases\n",
    "        will be updated) are accumulated in each backward pass.\n",
    "        https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch\n",
    "        \"\"\"\n",
    "        optimizer.zero_grad()  # SGD\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        # shape outputs: torch.Size([4, 10])\n",
    "        # for every image a prediction\n",
    "        #print(f\"{inputs}\")\n",
    "        outputs = net(inputs)\n",
    "        #print(f\"{outputs} \\t {labels}\")\n",
    "\n",
    "        # the first iteration CrossEntropy: tensor(2.3100, grad_fn=<NllLossBackward0>)\n",
    "        loss = criterion(outputs, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # running loss after 3 iterations: 6.894119024276733\n",
    "        # Why is the loss added?\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1}] loss: {(running_loss / 99):.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluate the model on the test data\n",
    "This could be done with TorchMetrics but we will do this manually here"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 1392 test embeddings: 97.70\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, labels = torch.max(labels.data, 1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the {len(test_labels)} test embeddings: {(100 * correct / total):.2f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class G_SP  is: 95.5 %\n",
      "Accuracy for class G     is: 99.7 %\n",
      "Accuracy for class SP_TM is: 91.7 %\n",
      "Accuracy for class TM    is: 100.0 %\n"
     ]
    },
    {
     "data": {
      "text/plain": "'\\nAccuracy for class G_SP  is: 97.7 %\\nAccuracy for class G     is: 99.3 %\\nAccuracy for class SP_TM is: 79.4 %\\nAccuracy for class TM    is: 86.2 %\\n'"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare to count predictions for each class\n",
    "classes = list(label_mappings.keys())\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        _, labels = torch.max(labels, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(\"Accuracy for class {:5s} is: {:.1f} %\".format(classname,\n",
    "                                                         accuracy))\n",
    "\n",
    "\"\"\"\n",
    "Accuracy for class G_SP  is: 97.7 %\n",
    "Accuracy for class G     is: 99.3 %\n",
    "Accuracy for class SP_TM is: 79.4 %\n",
    "Accuracy for class TM    is: 86.2 %\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create hashsum"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}