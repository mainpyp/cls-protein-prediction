{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append('../common')\n",
    "from dataset import TMH\n",
    "from models import CNN\n",
    "from cait_models import cait_S24_224, cait_tmh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../seq_anno_hash.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/r_/1k4kyy6s403b1fywyk7b3xz00000gn/T/ipykernel_77263/1302236751.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m dataset = TMH(embeddings_path=\"/Users/fga/data/tmh/embeddings.h5\",\n\u001b[0m\u001b[1;32m      2\u001b[0m                 \u001b[0mprotein_hashes_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"../seq_anno_hash.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                 train_ids=\"../data_splits/train_prot_id_labels.csv\")\n\u001b[1;32m      4\u001b[0m \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Docs/cls-protein-prediction/notebooks/../common/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, embeddings_path, protein_hashes_path, train_ids)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0membeddings_prot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotein_hashes_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mproteins_and_hashes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../seq_anno_hash.pickle'"
     ]
    }
   ],
   "source": [
    "dataset_path = \"/Users/fga/data/tmh\"\n",
    "dataset = TMH(embeddings_path=dataset_path+\"/embeddings.h5\",\n",
    "                protein_hashes_path=dataset_path+\"/seq_anno_hash.pickle\",\n",
    "                train_ids=dataset_path+\"/data_splits/train_prot_id_labels.csv\")\n",
    "dataloader = DataLoader(dataset, batch_size=4)\n",
    "\n",
    "test_dataset = TMH(embeddings_path=dataset_path+\"/Users/fga/data/tmh/embeddings.h5\",\n",
    "                protein_hashes_path=dataset_path+\"/seq_anno_hash.pickle\",\n",
    "                train_ids=dataset_path+\"/data_splits/test_prot_id_labels.csv\")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# net = CNN()\n",
    "# net = cait_S24_224()\n",
    "net = cait_tmh()\n",
    "\n",
    "# The function is indicating the performance of the model.\n",
    "# During the training process this function should be minimized\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# The minimization is achieved through Stochastic Gradient Descent\n",
    "optimizer = optim.Adagrad(net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4887,  0.6972, -0.0885, -0.3397],\n",
      "        [-0.4887,  0.6970, -0.0884, -0.3396],\n",
      "        [-0.4887,  0.6971, -0.0884, -0.3396],\n",
      "        [-0.4889,  0.6971, -0.0884, -0.3397]], grad_fn=<AddmmBackward0>) \t tensor([[0, 1, 0, 0],\n",
      "        [0, 0, 1, 0],\n",
      "        [0, 1, 0, 0],\n",
      "        [1, 0, 0, 0]]) \t 1.2423670291900635\n"
     ]
    }
   ],
   "source": [
    "# single step\n",
    "inputs, labels = iter(dataloader).__next__()\n",
    "\n",
    "x = inputs.unsqueeze(1)\n",
    "\n",
    "try:\n",
    "    outputs = net(x)\n",
    "except ValueError:\n",
    "    import ipdb\n",
    "    ipdb.post_mortem()\n",
    "\n",
    "loss = criterion(outputs, labels.float())\n",
    "\n",
    "print(f\"{outputs} \\n {labels} \\n {loss}\")\n",
    "\n",
    "# TODO: train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 100] loss: 1.187\n",
      "[1, 200] loss: 1.053\n",
      "[1, 300] loss: 1.056\n",
      "[1, 400] loss: 1.092\n",
      "[1, 500] loss: 1.132\n",
      "[1, 600] loss: 1.108\n",
      "[1, 700] loss: 1.023\n",
      "[1, 800] loss: 1.001\n",
      "[2, 100] loss: 0.872\n",
      "[2, 200] loss: 0.716\n",
      "[2, 300] loss: 0.600\n",
      "[2, 400] loss: 0.521\n",
      "[2, 500] loss: 0.479\n",
      "[2, 600] loss: 0.416\n",
      "[2, 700] loss: 0.350\n",
      "[2, 800] loss: 0.344\n",
      "[3, 100] loss: 0.275\n",
      "[3, 200] loss: 0.241\n",
      "[3, 300] loss: 0.245\n",
      "[3, 400] loss: 0.231\n",
      "[3, 500] loss: 0.238\n",
      "[3, 600] loss: 0.217\n",
      "[3, 700] loss: 0.218\n",
      "[3, 800] loss: 0.222\n",
      "[4, 100] loss: 0.171\n",
      "[4, 200] loss: 0.163\n",
      "[4, 300] loss: 0.178\n",
      "[4, 400] loss: 0.171\n",
      "[4, 500] loss: 0.179\n",
      "[4, 600] loss: 0.157\n",
      "[4, 700] loss: 0.177\n",
      "[4, 800] loss: 0.179\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(4):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        \"\"\"\n",
    "        shape of inputs: torch.Size([4, 3, 32, 32])\n",
    "            Batchsize: 4\n",
    "            Channels: 3 (Red, Green, Blue)\n",
    "            Image size: 32 x 32\n",
    "\n",
    "        labels: tensor([9, 3, 0, 3])\n",
    "            9: class of image 0 in batch\n",
    "            3: class of image 1 in batch\n",
    "            ...\n",
    "        \"\"\"\n",
    "        inputs, labels = data\n",
    "\n",
    "        \"\"\" zero the parameter gradients after every batch\n",
    "        This is necessary because the gradients (directions of how the weigths and biases\n",
    "        will be updated) are accumulated in each backward pass.\n",
    "        https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch\n",
    "        \"\"\"\n",
    "        optimizer.zero_grad()  # SGD\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        # shape outputs: torch.Size([4, 10])\n",
    "        # for every image a prediction\n",
    "        # print(f\"{inputs}\")\n",
    "        outputs = net(inputs.unsqueeze(1))\n",
    "\n",
    "        #print(f\"{outputs} \\t {labels}\")\n",
    "\n",
    "        # the first iteration CrossEntropy: tensor(2.3100, grad_fn=<NllLossBackward0>)\n",
    "\n",
    "        loss = criterion(outputs, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # running loss after 3 iterations: 6.894119024276733\n",
    "        # Why is the loss added?\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1}] loss: {(running_loss / 99):.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Evaluate the model on the test data\n",
    "This could be done with TorchMetrics but we will do this manually here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 1392 test embeddings: 96.05\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images.unsqueeze(1))\n",
    "\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, labels = torch.max(labels.data, 1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the {len(test_labels)} test embeddings: {(100 * correct / total):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class G_SP  is: 93.2 %\n",
      "Accuracy for class G     is: 99.4 %\n",
      "Accuracy for class SP_TM is: 87.6 %\n",
      "Accuracy for class TM    is: 93.5 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nAccuracy for class G_SP  is: 0.0 %\\nAccuracy for class G     is: 100.0 %\\nAccuracy for class SP_TM is: 0.0 %\\nAccuracy for class TM    is: 0.0 %\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare to count predictions for each class\n",
    "classes = list(label_mappings.keys())\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images.unsqueeze(1))\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        _, labels = torch.max(labels, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(\"Accuracy for class {:5s} is: {:.1f} %\".format(classname,\n",
    "                                                         accuracy))\n",
    "\n",
    "\"\"\"\n",
    "Accuracy for class G_SP  is: 93.2 %\n",
    "Accuracy for class G     is: 99.4 %\n",
    "Accuracy for class SP_TM is: 87.6 %\n",
    "Accuracy for class TM    is: 93.5 %\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Create hashsum"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
