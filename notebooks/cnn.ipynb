{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append('../common')\n",
    "from dataset import TMH\n",
    "from models import CNN\n",
    "from cait_models import cait_S24_224, cait_tmh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fga/Docs/cls-protein-prediction/notebooks/../common/dataset.py:47: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  embeddings = torch.Tensor(embeddings)\n"
     ]
    }
   ],
   "source": [
    "dataset = TMH(embeddings_path=\"/Users/fga/data/tmh/embeddings.h5\",\n",
    "                protein_hashes_path=\"../seq_anno_hash.pickle\",\n",
    "                train_ids=\"../data_splits/train_prot_id_labels.csv\")\n",
    "dataloader = DataLoader(dataset, batch_size=4)\n",
    "\n",
    "test_dataset = TMH(embeddings_path=\"/Users/fga/data/tmh/embeddings.h5\",\n",
    "                protein_hashes_path=\"../seq_anno_hash.pickle\",\n",
    "                train_ids=\"../data_splits/test_prot_id_labels.csv\")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# net = CNN()\n",
    "# net = cait_S24_224()\n",
    "net = cait_tmh()\n",
    "\n",
    "# The function is indicating the performance of the model.\n",
    "# During the training process this function should be minimized\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# The minimization is achieved through Stochastic Gradient Descent\n",
    "optimizer = optim.Adagrad(net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3019,  0.9502,  0.2833,  ..., -0.3675,  0.4257, -0.6928],\n",
      "        [ 0.3020,  0.9502,  0.2832,  ..., -0.3674,  0.4258, -0.6928],\n",
      "        [ 0.3019,  0.9503,  0.2834,  ..., -0.3674,  0.4257, -0.6929],\n",
      "        [ 0.3019,  0.9503,  0.2832,  ..., -0.3674,  0.4259, -0.6929]],\n",
      "       grad_fn=<AddmmBackward0>) \t tensor([[0, 1, 0, 0],\n",
      "        [0, 0, 1, 0],\n",
      "        [0, 1, 0, 0],\n",
      "        [1, 0, 0, 0]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1000])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# single step\n",
    "inputs, labels = iter(dataloader).__next__()\n",
    "\n",
    "x = inputs.unsqueeze(1)\n",
    "\n",
    "try:\n",
    "    outputs = net(x)\n",
    "except ValueError:\n",
    "    import ipdb\n",
    "    ipdb.post_mortem()\n",
    "\n",
    "# print(f\"{outputs} \\t {labels}\")\n",
    "outputs.shape\n",
    "\n",
    "# loss = criterion(outputs, labels.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LayerScale_Block(\n",
       "  (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "  (attn): Attention_talking_head(\n",
       "    (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "    (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (proj_l): Linear(in_features=8, out_features=8, bias=True)\n",
       "    (proj_w): Linear(in_features=8, out_features=8, bias=True)\n",
       "    (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (drop_path): Identity()\n",
       "  (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "  (mlp): Mlp(\n",
       "    (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "    (act): GELU()\n",
       "    (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.blocks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 100] loss: 1.187\n",
      "[1, 200] loss: 1.053\n",
      "[1, 300] loss: 1.056\n",
      "[1, 400] loss: 1.092\n",
      "[1, 500] loss: 1.132\n",
      "[1, 600] loss: 1.108\n",
      "[1, 700] loss: 1.023\n",
      "[1, 800] loss: 1.001\n",
      "[2, 100] loss: 0.872\n",
      "[2, 200] loss: 0.716\n",
      "[2, 300] loss: 0.600\n",
      "[2, 400] loss: 0.521\n",
      "[2, 500] loss: 0.479\n",
      "[2, 600] loss: 0.416\n",
      "[2, 700] loss: 0.350\n",
      "[2, 800] loss: 0.344\n",
      "[3, 100] loss: 0.275\n",
      "[3, 200] loss: 0.241\n",
      "[3, 300] loss: 0.245\n",
      "[3, 400] loss: 0.231\n",
      "[3, 500] loss: 0.238\n",
      "[3, 600] loss: 0.217\n",
      "[3, 700] loss: 0.218\n",
      "[3, 800] loss: 0.222\n",
      "[4, 100] loss: 0.171\n",
      "[4, 200] loss: 0.163\n",
      "[4, 300] loss: 0.178\n",
      "[4, 400] loss: 0.171\n",
      "[4, 500] loss: 0.179\n",
      "[4, 600] loss: 0.157\n",
      "[4, 700] loss: 0.177\n",
      "[4, 800] loss: 0.179\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(4):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        \"\"\"\n",
    "        shape of inputs: torch.Size([4, 3, 32, 32])\n",
    "            Batchsize: 4\n",
    "            Channels: 3 (Red, Green, Blue)\n",
    "            Image size: 32 x 32\n",
    "\n",
    "        labels: tensor([9, 3, 0, 3])\n",
    "            9: class of image 0 in batch\n",
    "            3: class of image 1 in batch\n",
    "            ...\n",
    "        \"\"\"\n",
    "        inputs, labels = data\n",
    "\n",
    "        \"\"\" zero the parameter gradients after every batch\n",
    "        This is necessary because the gradients (directions of how the weigths and biases\n",
    "        will be updated) are accumulated in each backward pass.\n",
    "        https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch\n",
    "        \"\"\"\n",
    "        optimizer.zero_grad()  # SGD\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        # shape outputs: torch.Size([4, 10])\n",
    "        # for every image a prediction\n",
    "        # print(f\"{inputs}\")\n",
    "        outputs = net(inputs.unsqueeze(1))\n",
    "\n",
    "        #print(f\"{outputs} \\t {labels}\")\n",
    "\n",
    "        # the first iteration CrossEntropy: tensor(2.3100, grad_fn=<NllLossBackward0>)\n",
    "\n",
    "        loss = criterion(outputs, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # running loss after 3 iterations: 6.894119024276733\n",
    "        # Why is the loss added?\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1}] loss: {(running_loss / 99):.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Evaluate the model on the test data\n",
    "This could be done with TorchMetrics but we will do this manually here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 1392 test embeddings: 96.05\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images.unsqueeze(1))\n",
    "\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, labels = torch.max(labels.data, 1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the {len(test_labels)} test embeddings: {(100 * correct / total):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class G_SP  is: 93.2 %\n",
      "Accuracy for class G     is: 99.4 %\n",
      "Accuracy for class SP_TM is: 87.6 %\n",
      "Accuracy for class TM    is: 93.5 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nAccuracy for class G_SP  is: 0.0 %\\nAccuracy for class G     is: 100.0 %\\nAccuracy for class SP_TM is: 0.0 %\\nAccuracy for class TM    is: 0.0 %\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare to count predictions for each class\n",
    "classes = list(label_mappings.keys())\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images.unsqueeze(1))\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        _, labels = torch.max(labels, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(\"Accuracy for class {:5s} is: {:.1f} %\".format(classname,\n",
    "                                                         accuracy))\n",
    "\n",
    "\"\"\"\n",
    "Accuracy for class G_SP  is: 93.2 %\n",
    "Accuracy for class G     is: 99.4 %\n",
    "Accuracy for class SP_TM is: 87.6 %\n",
    "Accuracy for class TM    is: 93.5 %\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Create hashsum"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
